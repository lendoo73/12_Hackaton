{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Template notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's good to start with an introduction, to set the scene and introduce your audience to the data, and the problem you're solving as a team.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Libraries\n",
    "As always, we'll start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas matplotlib seaborn ipykernel plotly nbformat scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation, it is necessary to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# It's good practice to add comments to explain your code \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question / Task 1**\n",
    "\n",
    "Insert context about question / task 1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "df = pd.read_csv(\"data/corona_tested_individuals_ver_006.english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_indication\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the Dataset\n",
    "\n",
    "The dataset consists of the following features for COVID-19 test records:\n",
    "\n",
    "* **`test_date`**: The date when the test was conducted.\n",
    "* **`cough`**: Binary value indicating the presence (1) or absence (0) of a cough.\n",
    "* **`fever`**: Binary value indicating the presence (1) or absence (0) of a fever.\n",
    "* **`sore_throat`**: Binary value indicating the presence (1) or absence (0) of a sore throat.\n",
    "* **`shortness_of_breath`**: Binary value indicating the presence (1) or absence (0) of shortness of breath.\n",
    "* **`head_ache`**: Binary value indicating the presence (1) or absence (0) of a headache.\n",
    "* **`corona_result`**: The result of the COVID-19 test, which can be 'negative', 'positive', or possibly 'other'.\n",
    "\n",
    "<details style=\"padding-left: 3rem\">\n",
    "    <summary>more details</summary>\n",
    "    <p>In the context of COVID-19 test results, the category \"other\" typically represents test outcomes that do not fall neatly into the binary categories of \"negative\" or \"positive.\" Here are some possible meanings for \"other\":</p>\n",
    "    <p>Indeterminate or Inconclusive: The test result was neither clearly positive nor negative. This can happen if the test sample was insufficient or contaminated.\n",
    "    Pending: The test result has not yet been finalized or reported.\n",
    "    Invalid: The test was not conducted properly, or there was an error in the testing process, leading to an invalid result.\n",
    "    Recovered: In some datasets, individuals who have previously tested positive and are now considered recovered may be categorized separately.\n",
    "    Understanding the exact meaning of \"other\" would require more detailed documentation or metadata from the dataset provider.</p>\n",
    "\n",
    "</details>\n",
    "\n",
    "* **`age_60_and_above`**: Indicator of whether the individual is aged 60 or above ('Yes', 'No'), with some missing values (NaN).\n",
    "* **`gender`**: The gender of the individual ('male' or 'female').\n",
    "* **`test_indication`**: The reason for taking the test, categorized as 'Contact with confirmed', 'Abroad', or 'Other'.\n",
    "\n",
    "The dataset captures various symptoms and demographic information along with COVID-19 test results, which can be used for exploratory data analysis and model building to predict COVID-19 test outcomes based on symptoms and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"corona_result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_indication\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### 1. About possible biases and limitations of this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection Method\n",
    "\n",
    "* **Source of Data**: The data is collected from a specific region, demographic, or population group, it might not be representative of the entire population. \n",
    "These data was collected on of all individuals in Israel tested for SARS-CoV-2 during the first months of the COVID-19 pandemic.\n",
    "* **Testing Access**: Individuals who have better access to healthcare facilities are more likely to get tested, which can lead to a selection bias.\n",
    "* **Symptom Reporting**: Self-reported symptoms can introduce bias due to underreporting or overreporting of symptoms. People might not report mild symptoms or may misreport symptoms due to fear or misunderstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Data\n",
    "\n",
    "* **`age_60_and_above`**: about 60% of data is missing. Missing data can introduce bias if the missingness is not random and is related to other variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Values\n",
    "\n",
    "* **Binary Representation of Symptoms**: The symptoms are represented as binary (0 or 1), which does not capture the severity of the symptoms. This simplification can lead to loss of information.\n",
    "* **`test_date`**: The dataset includes a `test_date`, but the relevance of this date to the onset of symptoms or to other temporal factors isn't clear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable (corona_result):\n",
    "\n",
    "* **Class Imbalance**: The dataset has a significant imbalance in the target variable (e.g., many more negative cases than positive cases), it can affect model performance and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Factors\n",
    "\n",
    "* **Temporal Changes**: The spread and detection of COVID-19 can change over time due to various factors like new variants, public health measures, and vaccination rates. If the data spans a long time period, these temporal changes can introduce bias.\n",
    "* **Behavioral Changes**: Changes in public behavior, such as mask-wearing and social distancing, can influence the likelihood of reporting certain symptoms and testing positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_indication\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format of Feature Values\n",
    "\n",
    "| Feature | Type | Format | Missing values |\n",
    "| :------ |:---- | :----- |:------- |\n",
    "| **`test_date`** | Date string | \"YYYY-MM-DD\", \"2020-04-30\" | 0 |\n",
    "| **`cough`** | Binary (Numeric) | 0.0 or 1.0 | 252 |\n",
    "| **`fever`** | Binary (Numeric) | 0.0 or 1.0 | 252 |\n",
    "| **`sore_throat`** | Binary (Numeric) | 0.0 or 1.0 | 0 |\n",
    "| **`shortness_of_breath`** | Binary (Numeric) | 0.0 or 1.0 | 1 |\n",
    "| **`head_ache`** | Binary (Numeric) | 0.0 or 1.0 | 1 |\n",
    "| **`corona_result`** | Categorical (String) | \"negative\", \"positive\", or \"other\"  | 0 |\n",
    "| **`age_60_and_above`** | Categorical (String) with missing values | \"Yes\", \"No\", or NaN  | 127,320 |\n",
    "| **`gender`** | Categorical (String) | \"male\" or \"female\"  | 19,563 |\n",
    "| **`test_indication`** | Categorical (String) | \"Contact with confirmed\", \"Abroad\", or \"Other\"  | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The statistics of feature values\n",
    "\n",
    "There are `278,848` entries in the dataset for all features.\n",
    "\n",
    "#### 3.1`test_date`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "\n",
    "* **Non-null Entries**: All entries are non-null, indicating that every record has a test date.\n",
    "* **Data Type**: The `test_date` is currently stored as an object (string), though it could be converted to a datetime type for more effective date-based operations and analysis.\n",
    "* **Frequency**: The highest number of tests was conducted on `2020-04-20` (10,921 tests) and the lowest on `2020-03-11` (294 tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date_counts = df[\"test_date\"].value_counts().sort_index()\n",
    "# test_date_counts.plot()\n",
    "fig = px.line(\n",
    "    test_date_counts, \n",
    "    x=test_date_counts.index,\n",
    "    y=test_date_counts.values,\n",
    "    title=\"Number of Tests Over Time\",\n",
    "    labels={\"index\": \"Test Date\", \"y\": \"Number of Tests\"}\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2`cough`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing values**: There are `252` missing values for the `cough` feature.\n",
    "- **Value Counts**:\n",
    "  - `0.0`: Reported in `236,368` instances.\n",
    "  - `1.0`: Reported in `42,228` instances.\n",
    "- **Prevalence**: Cough is reported in approximately `15.1%` of the total cases.\n",
    "- **Data Type Consideration**: `cough` is stored as a float64 (`0.0` and `1.0`), representing binary presence (`1.0`) or absence (`0.0`) of cough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df[\"cough\"].value_counts()\n",
    "value_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_value_count(feature, label=None):\n",
    "    value_counts = df[feature].value_counts()\n",
    "    missing = df[feature].isna().sum()\n",
    "    value_counts[\"missing\"] = missing\n",
    "    if not label:\n",
    "        label = value_counts.index.map({\n",
    "            0.0: f\"No {feature}: {value_counts.iloc[0]:,}\", \n",
    "            1.0: f\"{feature}: {value_counts.iloc[1]:,}\", \n",
    "            \"missing\": f\"missing: {missing}\"\n",
    "        })\n",
    "    else:\n",
    "        label = value_counts.index.map({\n",
    "            key: f\"{key}: {value_counts[key]:,}\" for key in value_counts.keys()\n",
    "        })\n",
    "    print(label)\n",
    "    data_for_pie = pd.DataFrame({\n",
    "        'value_counts': value_counts.values,\n",
    "        'status': label,\n",
    "        \"missing\": value_counts[\"missing\"]\n",
    "    })\n",
    "    \n",
    "    fig = px.pie(\n",
    "        data_for_pie,\n",
    "        values=\"value_counts\",\n",
    "        names=\"status\",\n",
    "        title=f\"Distribution of {feature} Feature\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title_x=0.5\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "pie_value_count(\"cough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3`fever`\n",
    "\n",
    "- **Missing values**: There are `252` missing values.\n",
    "- **Value Counts**:\n",
    "  - `0.0`: Reported in `256,844` instances.\n",
    "  - `1.0`: Reported in `21,752` instances.\n",
    "- **Prevalence**: Fever is reported in approximately `7.8%` of the total cases.\n",
    "- **Data Type Consideration**: `fever` is stored as a float64(`0.0` and `1.0`), representing binary presence (`1.0`) or absence (`0.0`) of fever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"fever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 `sore_throat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Missing values**: There are no missing values.\n",
    "- **Value Counts**:\n",
    "  - `0.0`: Reported in `276,921` instances.\n",
    "  - `1.0`: Reported in `1,926` instances.\n",
    "- **Prevalence**: Sore throat is reported in approximately `0.7%` of the total cases (`1,926 / 278,848`).\n",
    "- **Data Type Consideration**: `sore_throat` is stored as a float64 (`0.0` and `1.0`), representing binary presence (`1.0`) or absence (`0.0`) of sore throat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"sore_throat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 `shortness_of_breath`\n",
    "\n",
    "- **Missing values**: There is `1` missing value.\n",
    "- **Value Counts**:\n",
    "  - `0.0`: Reported in `277,270` instances.\n",
    "  - `1.0`: Reported in `1,577` instances.\n",
    "- **Prevalence**: Shortness of breath is reported in approximately `0.6%` of the total cases (`1,577 / 278,848`).\n",
    "- **Data Type Consideration**: `shortness_of_breath` is stored as a float64 (`0.0` and `1.0`), representing binary presence (`1.0`) or absence (`0.0`) of shortness of breath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"shortness_of_breath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 `head_ache`\n",
    "\n",
    "- **Missing values**: There is `1` missing value.\n",
    "- **Value Counts**:\n",
    "  - `0.0`: Reported in `276,433` instances.\n",
    "  - `1.0`: Reported in `2,414` instances.\n",
    "- **Prevalence**: Headache is reported in approximately `0.9%` of the total cases (`2,414 / 278,848`).\n",
    "- **Data Type Consideration**: `head_ache` is stored as a float64 (`0.0` and `1.0`), representing binary presence (`1.0`) or absence (`0.0`) of headache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"head_ache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 `corona_result`\n",
    "\n",
    "- **Missing values**: There are no missing values.\n",
    "- **Value Counts**:\n",
    "  - `negative`: Reported in `260,227` instances.\n",
    "  - `positive`: Reported in `14,729` instances.\n",
    "  - `other`: Reported in `3,892` instances.\n",
    "- **Distribution**:\n",
    "  - `negative`: Approximately `93.3%`.\n",
    "  - `positive`: Approximately `5.3%`.\n",
    "  - `other`: Approximately `1.4%`.\n",
    "- **Data Type Consideration**: `corona_result` is stored as an object (string), indicating the test result categories (`negative`, `positive`, `other`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"corona_result\", label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 `age_60_and_above`\n",
    "\n",
    "- **Missing values**: There are `127,320` missing values.\n",
    "- **Value Counts**:\n",
    "  - `No`: Reported in `125,703` instances.\n",
    "  - `Yes`: Reported in `25,825` instances.\n",
    "- **Distribution**:\n",
    "  - `No`: Approximately `83.0%`.\n",
    "  - `Yes`: Approximately `17.0%`.\n",
    "- **Data Type Consideration**: `age_60_and_above` is stored as an object (string), indicating binary categories (`No` and `Yes`) for age above 60 years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"age_60_and_above\", label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 `gender`\n",
    "\n",
    "- **Missing values**: There are `19,563` missing values.\n",
    "- **Value Counts**:\n",
    "  - `female`: Reported in `130,158` instances.\n",
    "  - `male`: Reported in `129,127` instances.\n",
    "- **Distribution**:\n",
    "  - `female`: Approximately `50.2%`.\n",
    "  - `male`: Approximately `49.8%`.\n",
    "- **Data Type Consideration**: `gender` is stored as an object (string), indicating binary categories (`female` and `male`) for gender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"gender\", label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 `test_indication`\n",
    "\n",
    "- **Missing values**: There are `0` missing values.\n",
    "- **Value Counts**:\n",
    "  - `Other`: Reported in `242,741` instances.\n",
    "  - `Abroad`: Reported in `25,468` instances.\n",
    "  - `Contact with confirmed`: Reported in `10,639` instances.\n",
    "- **Data Type Consideration**: `test_indication` is stored as an object (string), categorizing reasons for COVID-19 testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_value_count(\"test_indication\", label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Features grouped by the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, map all categorical features to binary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_indication\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(df):\n",
    "    df_filtered = df[df[\"corona_result\"] != \"other\"]\n",
    "\n",
    "    # drop 'test_data' and age_60_and_above (we have too many missing values)\n",
    "    df_filtered = df_filtered.drop(columns=[\"test_date\", \"age_60_and_above\"])\n",
    "\n",
    "\n",
    "    df_filtered[\"corona_result\"] = df[\"corona_result\"].map({\n",
    "        \"negative\": 0, \n",
    "        \"positive\": 1\n",
    "    })\n",
    "    df_filtered[\"gender\"] = df[\"gender\"].map({\n",
    "        \"male\": 0, \n",
    "        \"female\": 1\n",
    "    })\n",
    "    df_filtered[\"test_indication\"] = df[\"test_indication\"].map({\n",
    "        \"Other\": 1, \n",
    "        \"Abroad\": 2,\n",
    "        \"Contact with confirmed\": 3\n",
    "    })\n",
    "    df_filtered = df_filtered.dropna().astype(int)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = filter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null\n",
    "df_filtered.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_filtered.groupby('corona_result').sum().transpose()\n",
    "fig = px.bar(\n",
    "    grouped,\n",
    "    text_auto='.2s',\n",
    "    title=\"Features Count for Corona Result\"\n",
    ")\n",
    "fig.update_xaxes(title_text='Feature')\n",
    "fig.update_yaxes(title_text='Count')\n",
    "# Set barmode to 'group' for side-by-side bars\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title_x=0.5\n",
    ")\n",
    "# Map legend labels\n",
    "# Update trace names (legend labels)\n",
    "fig.update_traces(\n",
    "    name=\"negative\",  # For corona_result 0\n",
    "    selector={\"name\": \"0\"}\n",
    ")\n",
    "fig.update_traces(\n",
    "    name=\"positive\",  # For corona_result 1\n",
    "    selector={\"name\": \"1\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(df):\n",
    "    df_dummies = pd.get_dummies(\n",
    "        df, \n",
    "        columns=['test_indication'],\n",
    "        drop_first=True,\n",
    "        dtype=int\n",
    "    )\n",
    "    df_dummies.rename(\n",
    "        columns={'test_indication_2': 'test_indication_Abroad', 'test_indication_3': 'test_indication_Contact with confirmed'}, \n",
    "        inplace=True\n",
    "    )\n",
    "    return df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = dummy(df_filtered)\n",
    "df_dummies.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummies.drop(columns=[\"corona_result\"])\n",
    "y = df_dummies[\"corona_result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [0] * len(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return [[1, 0]] * len(X)  # Probability distribution for negative class\n",
    "\n",
    "base_model = BaseModel()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_pred = base_model.predict(X_test)\n",
    "base_pred_proba = base_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(base_pred_proba, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "rf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(rf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "precision_0 = report['0']['precision']\n",
    "recall_0 = report['0']['recall']\n",
    "f1_score_0 = report['0']['f1-score']\n",
    "support_0 = report['0']['support']\n",
    "\n",
    "# Extract metrics for class 1\n",
    "precision_1 = report['1']['precision']\n",
    "recall_1 = report['1']['recall']\n",
    "f1_score_1 = report['1']['f1-score']\n",
    "support_1 = report['1']['support']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "green = '\\033[92m'\n",
    "reset_color = '\\033[0m'\n",
    "\n",
    "print(f\"\"\"\n",
    "{green}Classification Report Interpretation:{reset_color}\n",
    "    \n",
    "{green}Precision:{reset_color} Precision measures the accuracy of positive predictions. \n",
    "    For class 0 (negative cases), the precision is {green}{precision_0:.2f}{reset_color}{reset_color}, \n",
    "        indicating that {precision_0 * 100:.0f}% of the samples predicted as negative were actually negative. \n",
    "    For class 1 (positive cases), the precision is {green}{precision_1:.2f}{reset_color}, \n",
    "        meaning that {precision_1 * 100:.0f}% of the samples predicted as positive were actually positive.\n",
    "\n",
    "{green}Recall (Sensitivity):{reset_color} Recall measures the proportion of actual positives that are correctly identified by the model. \n",
    "    For class 0 (negative cases), the recall is {green}{recall_0:.2f}{reset_color}, \n",
    "        indicating that {recall_0 * 100:.0f}% of the actual negative samples were correctly identified as negative. \n",
    "    For class 1 (positive cases), the recall is {green}{recall_1:.2f}{reset_color}, \n",
    "        meaning that {recall_1 * 100:.0f}% of the actual positive samples were correctly identified as positive.\n",
    "\n",
    "{green}F1-score:{reset_color} The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both measures.\n",
    "    For class 0, the F1-score is {green}{f1_score_0:.2f}{reset_color}, \n",
    "    and for class 1, it is {green}{f1_score_1:.2f}{reset_color}.\n",
    "\n",
    "{green}Support:{reset_color} Support refers to the number of actual occurrences of each class in the test set. \n",
    "    In this case, there are {green}{support_0}{reset_color} samples of class 0 and {green}{support_1}{reset_color} samples of class 1.\n",
    "\n",
    "{green}Accuracy:{reset_color} Overall accuracy of the model is {green}{accuracy:.2f}{reset_color}, \n",
    "    meaning that {accuracy * 100:.0f}% of the predictions made by the model are correct.\n",
    "\n",
    "{green}Macro Avg:{reset_color} The macro average calculates the average of the metrics (precision, recall, F1-score) for all classes without considering class imbalance. \n",
    "Here, the macro average F1-score is {green}{report['macro avg']['f1-score']:.2f}.\n",
    "\n",
    "{green}Weighted Avg:{reset_color} The weighted average calculates the average of the metrics, \n",
    "    weighted by support (the number of true instances for each label). \n",
    "    It gives more weight to the metrics of the majority class (class 0, negative cases). \n",
    "    Here, the weighted average F1-score is {green}{report['weighted avg']['f1-score']:.2f}{reset_color}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    rf_confusion_matrix,\n",
    "    text_auto=True,\n",
    "    labels={\n",
    "        \"x\": \"Predicted Label\",\n",
    "        \"y\": \"Actual Label\",\n",
    "        \"color\": \"Count\"\n",
    "    },\n",
    "    x=[\"Negative\", \"Positive\"],\n",
    "    y=[\"Negative\", \"Positive\"],\n",
    "    title=\"Confusion Matrix for Random Forest Model\"\n",
    ")\n",
    "for i in range(2):\n",
    "    fig.add_shape(type=\"line\", x0=0.5 + i, y0=-0.5, x1=0.5 + i, y1=2 - 0.5, line=dict(color=\"white\", width=2))\n",
    "    fig.add_shape(type=\"line\", x0=-0.5, y0=0.5 + i, x1=2 - 0.5, y1=0.5 + i, line=dict(color=\"white\", width=2))\n",
    "\n",
    "fig.update_layout(title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stratify:  \n",
    "[[48053   418]  \n",
    " [ 1096  1567]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{green}Confusion Matrix Interpretation:{reset_color}\n",
    "{reset_color}\n",
    "The confusion matrix provides a more detailed breakdown of predictions versus actual outcomes:\n",
    "\n",
    "{green}True Negative (TN):{reset_color} {rf_confusion_matrix[0][0]:,} samples were correctly predicted as negative.\n",
    "{green}False Positive (FP):{reset_color} {rf_confusion_matrix[0][1]:,} samples were incorrectly predicted as positive.\n",
    "{green}False Negative (FN):{reset_color} {rf_confusion_matrix[1][0]:,} samples were incorrectly predicted as negative (actually positive).\n",
    "{green}True Positive (TP):{reset_color} {rf_confusion_matrix[1][1]:,} samples were correctly predicted as positive.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Accuracy (ACC)</summary>\n",
    "\n",
    "$\\text{ACC} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{48,053 + 1,567}{48,053 + 418 + 1,096 + 1,567} \\approx 0.97 \\quad \\left(\\frac{\\text{TP} + \\text{TN}}{\\text{Total}} \\approx \\text{Accuracy}\\right)$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Precision for class 0 (Negative)</summary>\n",
    "\n",
    "$\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{48,053}{48,053 + 1,096} \\approx 0.98 \\quad \\left(\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} \\approx \\text{Precision for class 0}\\right)$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Precision for class 1 (Positive)</summary>\n",
    "\n",
    "$\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{1,567}{1,567 + 418} \\approx 0.79 \\quad \\left(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\approx \\text{Precision for class 1}\\right)$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Recall for class 0 (Negative)</summary>\n",
    "\n",
    "$\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{48,053}{48,053 + 418} \\approx 0.99 \\quad \\left(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \\approx \\text{Recall for class 0}\\right)$\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Recall for class 1 (Positive)</summary>\n",
    "\n",
    "$\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{1,567}{1,567 + 1,096} \\approx 0.59 \\quad \\left(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\approx \\text{Recall for class 1}\\right)$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(true, pred_proba, known=True):\n",
    "    roc_auc = roc_auc_score(true, pred_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(true, pred_proba)\n",
    "\n",
    "    fig = px.area(\n",
    "        x=fpr, y=tpr,\n",
    "        title=f'Random Forest ROC Curve using {\"Known\" if known else \"Unseen\"} Data (AUC={roc_auc:.4f})',\n",
    "        labels={\n",
    "            \"x\": \"False Positive Rate\", \n",
    "            \"y\": \"True Positive Rate\"\n",
    "        },\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stratify: 0.8976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`auPRC` Area under the Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "def plot_pr_curve(true, pred_proba, known=True):\n",
    "    precision, recall, _ = precision_recall_curve(true, pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    fig_pr = px.area(\n",
    "        x=recall, y=precision,\n",
    "        title=f'Random Forest Precision-Recall Curve {\"Known\" if known else \"Unseen\"} Data (auPRC={pr_auc:.4f})',\n",
    "        labels={\n",
    "            \"x\": \"Recall\", \n",
    "            \"y\": \"Precision\"\n",
    "        },\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig_pr.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig_pr.update_xaxes(constrain='domain')\n",
    "    fig_pr.update_layout(title_x=0.5)\n",
    "    fig_pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation using unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validate = pd.read_csv(\"data/corona_tested_individuals_ver_0083.english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the validation dataset\n",
    "df_validate_filtered = filter(df_validate)\n",
    "df_val_dummy = dummy(df_validate_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_dummy[\"test_indication_Contact with confirmed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = df_val_dummy.drop(columns=[\"corona_result\"])\n",
    "y_validate = df_val_dummy[\"corona_result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate_proba = rf_model.predict_proba(X_validate)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(y_validate, y_validate_proba, known=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_validate, y_validate_proba, known=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
